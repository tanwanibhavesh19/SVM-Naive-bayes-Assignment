{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical Questions**\n",
        "\n",
        "### **1. What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "**Answer:**\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding the optimal hyperplane that best separates the data points of different classes with the maximum margin. The data points closest to the hyperplane are called **support vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What is the difference between Hard Margin and Soft Margin SVM?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Hard Margin SVM:** Assumes that the data is linearly separable without any misclassification. It does not allow any errors.\n",
        "* **Soft Margin SVM:** Allows some misclassifications to prevent overfitting when data is not perfectly separable. Controlled by the parameter `C`.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What is the mathematical intuition behind SVM?**\n",
        "\n",
        "**Answer:**\n",
        "SVM aims to solve the optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{w, b} \\frac{1}{2} ||w||^2 \\quad \\text{subject to} \\quad y_i(w \\cdot x_i + b) \\geq 1\n",
        "$$\n",
        "\n",
        "It tries to maximize the margin $\\frac{2}{||w||}$ while ensuring that data points are correctly classified.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the role of Lagrange Multipliers in SVM?**\n",
        "\n",
        "**Answer:**\n",
        "Lagrange Multipliers are used to convert the constrained optimization problem into an unconstrained one by constructing the Lagrangian. This allows solving SVM using dual formulation, which is easier when applying kernels.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What are Support Vectors in SVM?**\n",
        "\n",
        "**Answer:**\n",
        "Support Vectors are the data points that lie closest to the separating hyperplane. They are critical in defining the position and orientation of the hyperplane.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. What is a Support Vector Classifier (SVC)?**\n",
        "\n",
        "**Answer:**\n",
        "SVC is a classification version of SVM. It finds the optimal hyperplane that separates different classes. It supports different kernel functions (linear, RBF, polynomial, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What is a Support Vector Regressor (SVR)?**\n",
        "\n",
        "**Answer:**\n",
        "SVR is the regression version of SVM. Instead of finding a separating hyperplane, it fits a line within a specified margin of tolerance. It tries to keep predictions within an ε-tube from the true values.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is the Kernel Trick in SVM?**\n",
        "\n",
        "**Answer:**\n",
        "The Kernel Trick allows SVM to operate in a high-dimensional space without explicitly computing coordinates. It calculates the dot product in the transformed space using a kernel function, enabling SVM to classify non-linear data.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Kernel     | Function                  | Use Case                          |       |   |       |                                  |\n",
        "| ---------- | ------------------------- | --------------------------------- | ----- | - | ----- | -------------------------------- |\n",
        "| Linear     | $K(x, y) = x^T y$         | Linearly separable data           |       |   |       |                                  |\n",
        "| Polynomial | $K(x, y) = (x^T y + c)^d$ | Captures polynomial relationships |       |   |       |                                  |\n",
        "| RBF        | ( K(x, y) = \\exp(-\\gamma  |                                   | x - y |   | ^2) ) | Captures complex non-linear data |\n",
        "\n",
        "---\n",
        "\n",
        "### **10. What is the effect of the C parameter in SVM?**\n",
        "\n",
        "**Answer:**\n",
        "The `C` parameter controls the trade-off between achieving a low training error and a large margin.\n",
        "\n",
        "* **High C** → Low bias, high variance (fewer margin violations)\n",
        "* **Low C** → High bias, low variance (more margin, more violations allowed)\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is the role of the Gamma parameter in RBF Kernel SVM?**\n",
        "\n",
        "**Answer:**\n",
        "Gamma defines the influence of a single training point.\n",
        "\n",
        "* **Low Gamma:** Farther points considered → smoother decision boundary\n",
        "* **High Gamma:** Only close points matter → more complex boundary\n",
        "\n",
        "---\n",
        "\n",
        "### **12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "**Answer:**\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that all features are conditionally independent given the class label—this is the \"naïve\" assumption.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What is Bayes’ Theorem?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "It allows calculating the posterior probability $P(A|B)$ given the prior $P(A)$, likelihood $P(B|A)$, and evidence $P(B)$.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Variant        | Use Case                          | Assumes                |\n",
        "| -------------- | --------------------------------- | ---------------------- |\n",
        "| Gaussian NB    | Continuous features               | Normal distribution    |\n",
        "| Multinomial NB | Text classification (word counts) | Discrete counts        |\n",
        "| Bernoulli NB   | Binary/boolean features           | Bernoulli distribution |\n",
        "\n",
        "---\n",
        "\n",
        "### **15. When should you use Gaussian Naïve Bayes over other variants?**\n",
        "\n",
        "**Answer:**\n",
        "Use Gaussian Naïve Bayes when features are continuous and roughly follow a normal distribution—e.g., sensor data, medical datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **16. What are the key assumptions made by Naïve Bayes?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* All features are conditionally independent given the class.\n",
        "* The data is representative of the underlying distribution.\n",
        "* Features contribute independently to the probability.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. What are the advantages and disadvantages of Naïve Bayes?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple and fast\n",
        "* Works well with high-dimensional data\n",
        "* Requires less training data\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Strong independence assumption\n",
        "* Less accurate with highly correlated features\n",
        "\n",
        "---\n",
        "\n",
        "### **18. Why is Naïve Bayes a good choice for text classification?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* Words (features) can be treated independently.\n",
        "* It handles high-dimensional sparse data efficiently.\n",
        "* Performs well even with small datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. Compare SVM and Naïve Bayes for classification tasks.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Criteria    | SVM                     | Naïve Bayes            |\n",
        "| ----------- | ----------------------- | ---------------------- |\n",
        "| Accuracy    | Often higher            | Moderate               |\n",
        "| Speed       | Slower                  | Very fast              |\n",
        "| Assumptions | Margin-based            | Feature independence   |\n",
        "| Data Type   | Numerical, text, image  | Mostly text or tabular |\n",
        "| Tuning      | Needs tuning (C, gamma) | Less tuning required   |\n",
        "\n",
        "---\n",
        "\n",
        "### **20. How does Laplace Smoothing help in Naïve Bayes?**\n",
        "\n",
        "**Answer:**\n",
        "Laplace Smoothing avoids assigning zero probability to unseen feature combinations by adding a small constant (usually 1) to all counts. It ensures the model can handle words not seen during training.\n"
      ],
      "metadata": {
        "id": "EWK6fa8WfdjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical Questions**"
      ],
      "metadata": {
        "id": "du5hCaBVf3PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hO0VFa91f8eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "svc_linear = SVC(kernel='linear')\n",
        "svc_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svc_linear.fit(X_train, y_train)\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", svc_linear.score(X_test, y_test))\n",
        "print(\"RBF Kernel Accuracy:\", svc_rbf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "26rK4kYmgqrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "6Tl4R7kagqoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, n_redundant=0, random_state=42)\n",
        "model = SVC(kernel='poly', degree=3)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plotting\n",
        "h = .02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.title(\"SVM with Polynomial Kernel\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3iBu8x7Ogqlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "RzESrRR7gqjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26: Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8KQo5yalgqhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27: Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, C_val in enumerate([0.1, 1, 10]):\n",
        "    clf = SVC(C=C_val, kernel='linear')\n",
        "    clf.fit(X, y)\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(f\"C = {C_val}\")\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dZe4Yc_dgqew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28: Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic binary dataset\n",
        "X = np.random.randint(2, size=(1000, 10))\n",
        "y = np.random.randint(2, size=(1000,))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "A-x58wCngqcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29: Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model_unscaled = SVC()\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = model_unscaled.score(X_test, y_test)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = SVC()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = model_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_unscaled)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "id": "sklUvJEogqZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30: Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Standard GaussianNB (Laplace Smoothing not applicable directly here)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy without Laplace Smoothing (default):\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Note: GaussianNB does not support Laplace smoothing because it's not based on count-based probabilities\n",
        "print(\"Laplace smoothing is generally used in Multinomial or Bernoulli Naïve Bayes.\")\n"
      ],
      "metadata": {
        "id": "cOxa-IoigqXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31: Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "WoMN_5ZJgqUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32: Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Without class weighting\n",
        "model_default = SVC()\n",
        "model_default.fit(X_train, y_train)\n",
        "y_pred_default = model_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# With class weighting\n",
        "model_weighted = SVC(class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"Accuracy without class weight:\", acc_default)\n",
        "print(\"Accuracy with class weight:\", acc_weighted)\n"
      ],
      "metadata": {
        "id": "AJAsYRCCgqSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33: Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Using two categories to simulate spam (comp.*) and not-spam (rec.*)\n",
        "categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball']\n",
        "data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"Spam Detection Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Uh_UtwL3gqPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34: Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "svm = SVC()\n",
        "nb = GaussianNB()\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "svm_pred = svm.predict(X_test)\n",
        "nb_pred = nb.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_pred))\n"
      ],
      "metadata": {
        "id": "Mub2fgL0gqMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35: Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Without feature selection\n",
        "model_all = GaussianNB()\n",
        "model_all.fit(X_train, y_train)\n",
        "acc_all = accuracy_score(y_test, model_all.predict(X_test))\n",
        "\n",
        "# With feature selection (top 10 features)\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "\n",
        "model_sel = GaussianNB()\n",
        "model_sel.fit(X_train_sel, y_train)\n",
        "acc_sel = accuracy_score(y_test, model_sel.predict(X_test_sel))\n",
        "\n",
        "print(\"Accuracy without feature selection:\", acc_all)\n",
        "print(\"Accuracy with feature selection:\", acc_sel)\n"
      ],
      "metadata": {
        "id": "wYV3HE02gqKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36: Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "ovr = OneVsRestClassifier(SVC())\n",
        "ovo = OneVsOneClassifier(SVC())\n",
        "\n",
        "ovr.fit(X_train, y_train)\n",
        "ovo.fit(X_train, y_train)\n",
        "\n",
        "ovr_acc = accuracy_score(y_test, ovr.predict(X_test))\n",
        "ovo_acc = accuracy_score(y_test, ovo.predict(X_test))\n",
        "\n",
        "print(\"One-vs-Rest Accuracy:\", ovr_acc)\n",
        "print(\"One-vs-One Accuracy:\", ovo_acc)\n"
      ],
      "metadata": {
        "id": "YKjFsSvGgqHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37: Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for kernel in kernels:\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{kernel.capitalize()} Kernel Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "4GiJz9t5gqEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38: Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model = SVC()\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "print(\"Cross-validation accuracies:\", scores)\n",
        "print(\"Average accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "D4kULfx2gqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39: Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Equal priors\n",
        "model_equal = GaussianNB(priors=[0.5, 0.5])\n",
        "model_equal.fit(X_train, y_train)\n",
        "acc_equal = accuracy_score(y_test, model_equal.predict(X_test))\n",
        "\n",
        "# Empirical priors (default)\n",
        "model_default = GaussianNB()\n",
        "model_default.fit(X_train, y_train)\n",
        "acc_default = accuracy_score(y_test, model_default.predict(X_test))\n",
        "\n",
        "print(\"Accuracy with equal priors:\", acc_equal)\n",
        "print(\"Accuracy with empirical priors:\", acc_default)\n"
      ],
      "metadata": {
        "id": "nkmAVp7Ngp_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40: Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Without RFE\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "acc_all = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "# With RFE (select top 10 features)\n",
        "selector = RFE(SVC(kernel=\"linear\"), n_features_to_select=10)\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "model_rfe = SVC()\n",
        "model_rfe.fit(X_train_rfe, y_train)\n",
        "acc_rfe = accuracy_score(y_test, model_rfe.predict(X_test_rfe))\n",
        "\n",
        "print(\"Accuracy without RFE:\", acc_all)\n",
        "print(\"Accuracy with RFE:\", acc_rfe)\n"
      ],
      "metadata": {
        "id": "z_yDhW8rgp8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41: Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "n16O3h4hgp6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42: Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "print(\"Log Loss:\", log_loss(y_test, y_proba))\n"
      ],
      "metadata": {
        "id": "ZglqaEJqgp3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43: Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix - SVM Classifier\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lc3SG0Yogp0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44: Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "model = SVR()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Ze_nU9_zgpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45: Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "id": "KbQcTv63gpvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46: Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train SVM with probability=True to get scores\n",
        "model = SVC(probability=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve - SVM Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V__VreQ2gps7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}